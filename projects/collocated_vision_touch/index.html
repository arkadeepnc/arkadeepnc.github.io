<!-- 
<!DOCTYPE html>
<html> -->

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Coll_Vis_Touch</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf/"/>
    <meta property="og:title" content="mip-NeRF" />
    <meta property="og:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
               Using Collocated Vision and Tactile Sensors <br> for Visual Servoing and Localization
                </br> 
                <small>
                    <!-- IRCA-RAL 22 -->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://arkadeepnc.github.io/">
                          Arkadeep Narayan Chaudhury
                        </a>
                        </br>CMU RI
                    </li>
                    <li>
                        <a href="">
                            Tim Man
                        </a>
                        </br>CMU RI
                    </li>
                    <li>
                        <a href="http://robotouch.ri.cmu.edu/yuanwz/">
                          Wenzhen Yuan
                        </a>
                        </br>CMU RI
                    </li><br>
                    <li>
                        <a href="http://www.cs.cmu.edu/~cga/">
                          Chris Atkeson
                        </a>
                        </br>CMU RI
                    </li>
                   </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="./img/ICRA_RAL_Formatted-compressed.pdf">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/GwIF3bNlPnw">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                                <h4><small>Coming soon... </small> </h4>
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/opening_fig.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Coordinating proximity and tactile imaging by collocating cameras with tactile sensors can 1) provide useful information before contact such as object pose estimates and visually servo a robot
                    to a target with reduced occlusion and higher resolution compared to head-mounted or external depth cameras,
                     2) simplify the contact point and pose estimation problems and help tactile sensing avoid erroneous matches when
                    a surface does not have significant texture or has repetitive texture with many possible matches, and 3) use tactile imaging to further refine contact point and object pose estimation.
                   We demonstrate our results  with objects that have more surface texture than most objects in standard manipulation datasets. 
                  We learn that optic flow needs to be integrated over a substantial amount of camera travel to be useful in predicting movement direction. Most importantly, we also learn that state of the art vision algorithms
                  do not do a good job localizing tactile images
                  on object models, unless a reasonable prior can be
                  provided from collocated cameras.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/GwIF3bNlPnw" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Slides
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTF8WNl8sdtoORXmqUvMbZ3P7SdG8ZqLD58cc56kIGrCCDPW2r79ZOBfRFFjSklwYoljHUeBczknIJD/embed?start=false&loop=false&delayms=15000" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%; mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visual Servoing with Collocated Cameras
                </h3>
                <p class="text-justify">
                    Here, we demonstrate how a pair of collocated cameras can be used to correct trajectory errors on the fly using the optic flow observed by the camera. As a use case, we start with a initial erroneous estimate of the workspace goal (in the world coordinates) and we use the optical flow observed by the cameras en-route to correct against refined estimates of the goal (specified in pixel space by an external agent like a separate algorithm or a human annotator). The key idea behind this is that if the current heading direction is correct, the point of expansion corresponding to the current movement of the cameras should correspond to the workspace goal (in 3d coordinates) imaged by the cameras. We first calculate the point of expansion and then the error (in pixel space) between the current heading direction and the imaged true workspace goal. This pixel space error is then upgraded to the trajectory error using our knowledge of the camera intrinsic and extrinsic parameters (with respect to the robot) and the trajectory error is corrected. 
                </p>
                <p class = "text-justify">
                    We provide the algorithm we used to identify the point in image space corresponding to the momentary heading direction of the robot below. We call this point as the Point of Expansion (POE).
                </p>
               <p style="text-align:center;">
                <image src="./img/POE_algo.png" height="40px" class="img-responsive">
                </p>
                <h3>
                    Correcting trajectory errors with POE estimates
                </h3>
                <p class="text-justify">
                    Let us denote the true workspace goal as \(\mathbf{X}_G^W \) and it's estimate \(\widehat{\mathbf{X}_G^W} \) in homogeneous 3 space. The robot is initially planned to move to \(\widehat{\mathbf{X}_G^W}\) and let the POEs obtained for the subsequent frames be \(\mathbf{p}\), in the pixel space. As the camera is registered to the robot, a given point in time, we can calculate the world to camera projection matrix as \(\mathbf{P} = \mathbf{K}[\mathbf{R}_{world}^{cam}|\mathbf{t}_{world}^{cam}]\)
                    and can project \(\mathbf{X}_G^W\) and it's estimate \(\widehat{\mathbf{X}_G^W}\) to \(\mathbf{x}_G\) and \(\widehat{\mathbf{x}_G}\) in homogeneous 2 space. If the motion between two frames captured by the camera-in-hand is mostly perpendicular to the imaging plane, we can approximate \(\widehat{\mathbf{x}_G}\) with the point of expansion \(\mathbf{p}\) for each subsequent frames. We also note that, as \(\mathbf{p}\) is in the pixel space, with this approximation, we discard the "z-buffer" of the projective transformations associated with \(\mathbf{x}_G\) and as a consequence, cannot correct for a error in the camera's projective axis (camera's Z axis). We  use the camera projection Jacobian and calculate the task-space error as below: 
                    $$
                    \Delta\mathbf{X} = \left[\begin{array}{ccc}
                        \frac{f_x}{Z_W} & 0 & -\frac{X^W_G f_x+Z^W_G c_x}{{Z^W_G}^2}+\frac{c_x}{Z^W_G} \\
                        0 & \frac{f_y}{Z^W_G} & -\frac{Y^W_G f_y+Z^W_G c_y}{{Z^W_G}^2}+\frac{y}{Z^W_G}
                    \end{array} \right]^+ \left[\begin{array}{c}
                    p^u -   \mathbf{x}_G^u   \\
                    p^v - \mathbf{x}_G^v
                    \end{array} \right]
                    $$
                    In the equation above, \([f_x, f_y, c_x, c_y]\) are the camera intrinsics, and we denote \(\mathbf{X}_G^W = [X^W_G, Y^W_G, Z^W_G]\) and the \([\cdot]^+\) denotes the Moore-Penrose pseudo-inverse. We use the following procedure to correct trajectory goals as the robot moves towards the true workspace goal \(\mathbf{X}_G^W\) with a erroneous initial estimate.  
                </p>              


            </div>
        </div> 
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
TBD</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-1">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank <a href="https://leonidk.com/"> Leonid Keselman </a>,  <a href="https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/">Oliver Kroemer</a>, <a href="https://beisner.me/">Ben Eisner</a>, Arpit Agarwal and <a href="https://sites.google.com/view/rishiv/">Rishi Veerapaneni</a> for several constructive discussions and feedback on the manuscript. This research was partially funded by the NSF.
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
