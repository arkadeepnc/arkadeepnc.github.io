<!-- 
<!DOCTYPE html>
<html> -->

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Coll_Vis_Touch</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf/"/>
    <meta property="og:title" content="mip-NeRF" />
    <meta property="og:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." /> -->

        <!--TWITTER-->
    <!-- <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
               Using Collocated Vision and Tactile Sensors <br> for Visual Servoing and Localization
                </br> 
                <small>
                    <!-- IRCA-RAL 22 -->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://arkadeepnc.github.io/">
                          Arkadeep Narayan Chaudhury
                        </a>
                        </br>CMU RI
                    </li>
                    <li>
                        <a href="">
                            Tim Man
                        </a>
                        </br>CMU RI
                    </li>
                    <li>
                        <a href="http://robotouch.ri.cmu.edu/yuanwz/">
                          Wenzhen Yuan
                        </a>
                        </br>CMU RI
                    </li><br>
                    <li>
                        <a href="http://www.cs.cmu.edu/~cga/">
                          Chris Atkeson
                        </a>
                        </br>CMU RI
                    </li>
                   </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2204.11686">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>ArXiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./img/ICRA_RAL_reviewer_responses_compressed.pdf">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Reviews</strong></h4>
                                <!-- <h4><small>Coming soon... </small> </h4> -->
                            </a>
                        </li> 
                        <li>
                            <a href="https://youtu.be/GwIF3bNlPnw">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                                <h4><small>Coming soon... </small> </h4>
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/opening_fig.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Coordinating proximity and tactile imaging by collocating cameras with tactile sensors can 1) provide useful information before contact such as object pose estimates and visually servo a robot
                    to a target with reduced occlusion and higher resolution compared to head-mounted or external depth cameras,
                     2) simplify the contact point and pose estimation problems and help tactile sensing avoid erroneous matches when
                    a surface does not have significant texture or has repetitive texture with many possible matches, and 3) use tactile imaging to further refine contact point and object pose estimation.
                   We demonstrate our results  with objects that have more surface texture than most objects in standard manipulation datasets. 
                  We learn that optic flow needs to be integrated over a substantial amount of camera travel to be useful in predicting movement direction. Most importantly, we also learn that state of the art vision algorithms
                  do not do a good job localizing tactile images
                  on object models, unless a reasonable prior can be
                  provided from collocated cameras.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/GwIF3bNlPnw" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Slides
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTF8WNl8sdtoORXmqUvMbZ3P7SdG8ZqLD58cc56kIGrCCDPW2r79ZOBfRFFjSklwYoljHUeBczknIJD/embed?start=false&loop=false&delayms=15000" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%; mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visual Servoing with Collocated Cameras
                </h3>
                <p class="text-justify">
                    Here, we demonstrate how a pair of collocated cameras can be used to correct trajectory errors on the fly using the optic flow observed by the camera. As a use case, we start with a initial erroneous estimate of the workspace goal (in the world coordinates) and we use the optical flow observed by the cameras en-route to correct against refined estimates of the goal (specified in pixel space by an external agent like a separate algorithm or a human annotator). The key idea behind this is that if the current heading direction is correct, the point of expansion corresponding to the current movement of the cameras should correspond to the workspace goal (in 3d coordinates) imaged by the cameras. We first calculate the point of expansion and then the error (in pixel space) between the current heading direction and the imaged true workspace goal. This pixel space error is then upgraded to the trajectory error using our knowledge of the camera intrinsic and extrinsic parameters (with respect to the robot) and the trajectory error is corrected. 
                </p>
                <p class = "text-justify">
                    We provide the algorithm we used to identify the point in image space corresponding to the momentary heading direction of the robot below. We call this point as the Point of Expansion (POE).
                </p>
               <p style="text-align:center;">
                <image src="./img/POE_algo.png" height="40px" class="img-responsive">
                </p>
                <h3>
                    Correcting trajectory errors with POE estimates
                </h3>
                <p class="text-justify">
                    Let us denote the true workspace goal as \(\mathbf{X}_G^W \) and it's estimate \(\widehat{\mathbf{X}_G^W} \) in homogeneous 3 space. The robot is initially planned to move to \(\widehat{\mathbf{X}_G^W}\) and let the POEs obtained for the subsequent frames be \(\mathbf{p}\), in the pixel space. As the camera is registered to the robot, a given point in time, we can calculate the world to camera projection matrix as \(\mathbf{P} = \mathbf{K}[\mathbf{R}_{world}^{cam}|\mathbf{t}_{world}^{cam}]\)
                    and can project \(\mathbf{X}_G^W\) and it's estimate \(\widehat{\mathbf{X}_G^W}\) to \(\mathbf{x}_G\) and \(\widehat{\mathbf{x}_G}\) in homogeneous 2 space. If the motion between two frames captured by the camera-in-hand is mostly perpendicular to the imaging plane, we can approximate \(\widehat{\mathbf{x}_G}\) with the point of expansion \(\mathbf{p}\) for each subsequent frames. We also note that, as \(\mathbf{p}\) is in the pixel space, with this approximation, we discard the "z-buffer" of the projective transformations associated with \(\mathbf{x}_G\) and as a consequence, cannot correct for a error in the camera's projective axis (camera's Z axis). We  use the camera projection Jacobian and calculate the task-space error as below: 
                    $$
                    \Delta\mathbf{X} = \left[\begin{array}{ccc}
                        \frac{f_x}{Z_W} & 0 & -\frac{X^W_G f_x+Z^W_G c_x}{{Z^W_G}^2}+\frac{c_x}{Z^W_G} \\
                        0 & \frac{f_y}{Z^W_G} & -\frac{Y^W_G f_y+Z^W_G c_y}{{Z^W_G}^2}+\frac{y}{Z^W_G}
                    \end{array} \right]^+ \left[\begin{array}{c}
                    p^u -   \mathbf{x}_G^u   \\
                    p^v - \mathbf{x}_G^v
                    \end{array} \right]
                    $$
                    In the equation above, \([f_x, f_y, c_x, c_y]\) are the camera intrinsics, and we denote \(\mathbf{X}_G^W = [X^W_G, Y^W_G, Z^W_G]\) and the \([\cdot]^+\) denotes the Moore-Penrose pseudo-inverse. We use the following procedure to correct trajectory goals as the robot moves towards the true workspace goal \(\mathbf{X}_G^W\) with a erroneous initial estimate.  
                </p>   
                
                <h3>
                    Empirical evidence of error averaging across 10 cm steps
                </h3>

                <p class="text-justify">
                    <p style="text-align:center;">
                        <image src="./img/all_POEs.png" height="40px" class="img-responsive">
                </p>
                </p>
                <p class="text-justify">
                    In this experiment, we move the robot vertically down by 65 cm to a goal location slightly below the yellow cross mark on the handle of the glue gun. There are no errors in the goal location being tracked for this case. This experiment is repeated 10 times. The red dots are the predictions of potential point of contact (calculated as the instantaneous POE). We note that the predictions are centered about the actual point of contact -- a point slightly below the yellow cross mark on the glue gun. We report 6 cases where we predict the  potential point of contact by looking at various intervals of the trajectory. We report the interval lengths (in cm) and the standard deviation in predicting the point of contact (in pixels) as the labels of the figures. We note that as we increase the length of the interval, the standard deviation of the prediction decreases (as seen through ``clumping" of the predicted potential points of contact), but the number of possible predictions decreases (as seen through fewer number of red dots with increasing interval sizes). This leads us to conclude that the averages across larger temporal (and spatial) windows produce smoother and more stable error signals (or correction signals in the case of visual servoing). For our use case, averaging across 10 cm intervals provided us with ``enough'' number of correction signals while having reasonably low variance.
                </p> 
            </div>
        </div> 

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p style="text-align:center;">
                    <image src="./img/all_object_cad.png" height="40px" class="img-responsive">
                </p>
                <p class="text-justify">
                    We used the following objects for our localization experiments. From top (L-R) a folding knife, a glue gun, a wooden clip, a monkey from the Barrel of Monkey's game, a circuit board, a box cutter and a textured metallic pin. A 10cm \(\times\) 1cm rectangle is inserted at the bottom for scale.
                </p>
                <p style="text-align:center;">
                 <image src="./img/repeated_expt_table.png" height="50px" class="img-responsive">
                 </p>
                <p class = "text-justify">
                    In the experiment above, we focus on the case where the contact point has a unique arrangement of useful tactile features. This is the best case for a combined approach. If the tactile sensor contacts a featureless area, that can  be detected and the tactile sensor output can be ignored. For each of the 6 objects used in this work, we fix the object to the robot table. Assuming that there is zero error in position control of our robot (we use a Universal Robots UR5E manipulator fixed to a vention.io
                    table of recommended design for the same robot), we register the object with respect to the robot base and treat this pose as our ground truth. Next, we move the robot vertically 1 m above the object and move the robot down to touch the object at a chosen point that will yield good tactile information, and
                    localize the object with respect to the robot. We repeat this 3
                    times for the same object positions and repeat this experiment
                    for 2 more positions of the object with respect to the robot --
                    i.e. localizing each object 9 times with respect to the robot.
                    Fixing the objects is a restrictive assumption in the context of
                    localizing objects especially with touch, however, to ensure
                    repeatability of the experiments reported in the section, we
                    had to fix the objects to a rigid base. Following recent literature
                    we report the repeatibility of our pose estimation pipeline as
                    the measure of its performance.
                    Using tactile sensing the localization errors were brought
                    down to ±1.5mm in translation and ±0.\(5^o\) in rotation from
                    about 1.5cm in translation and 2\(^o\) in rotation using only
                    vision 3 . However, for cases where the tactile features were
                    not unique, e.g. the box cutter teeth and the metallic slender pin
                    object, the tactile sensing actually increased the
                    localization errors in the horizontal directions. The order
                    of these errors were equivalent to the scale of the repeated
                    features -- 5mm for the box-cutter (teeth are about 3mm wide placed in intervals of
                    5mm) and about 2mm for the long textured metallic pin (the embossed features are very similar at intervals of 3.5 mm). This observation is consistent with the fact that the
                    final gradient descent step  to refine the camera based pose estimates will converge to the wrong local minima if the tactile measurements are not distinctive enough. 
                </p>

                <p style="text-align:center;">
                    <image src="./img/random_touch.png" height="50px" class="img-responsive">
                    </p>
                   <p class = "text-justify">
                    In this experiment we present the effect of randomly selected contacts for localization. For this set of experiments, we fix each of the objects and the black background plate used in experiments above to a graduated compound slide capable of in plane translation and rotation. We then moved the robot vertically down to make contact at the same location on the object used for the experiments reported in Table I to generate a starting pose. Next, we generated 5 random configurations per object in translation and orientation on the plane of the table and moved the robot vertically down to touch the object and attempted to recover the randomly generated pose perturbations we introduced. For each of the objects, as expected, we observed similar errors in localization using only vision as reported in Table I. For the box cutter most of the contacts yielded useful tactile signals so the errors in recovering the perturbations in pose were in the range reported in Table I -- i.e. \(\sim 8\)mm in translation and \(\sim 1.5^o\) in rotation. This observation was also consistent for the smaller textured objects (like the monkey, metallic in and the camera circuit). However, tactile sensing was not always helpful in localizing the objects -- for the glue gun and the folding knife, significant parts of the object were featureless and the tactile signals obtained when touched at these parts were unusable in localizing the objects as the final gradient descent step re-introduced localization errors of about \(\sim 3-4\) cm and \(15^o\) by converging to incorrect poses. In Table II, we provide a subset of the results of these experiments with possible causes behind the failures. 
                   </p>

                <p style="text-align:center;">
                    <image src="./img/small_flat_obj.png" height="35px" class="img-responsive">
                </p>
                <p style="text-align:center;">
                    <image src="./img/big_objs.png" height="35px" class="img-responsive">
                </p>
                <p style="text-align: center;"> Some tactile signals captured by our system and qualitative representation of the final result of our localization pipeline. </p>

            </div>
        </div> 
           
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <p class="text-justify">
                TBD
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank <a href="https://leonidk.com/"> Leonid Keselman </a>,  <a href="https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/">Oliver Kroemer</a>, <a href="https://beisner.me/">Ben Eisner</a>, <a href = 'https://www.andrew.cmu.edu/user/arpita1/'>Arpit Agarwal</a> and <a href="https://sites.google.com/view/rishiv/">Rishi Veerapaneni</a> for several constructive discussions and feedback on the manuscript. This research was partially funded by the NSF and the Toyota Research Institute.
                The website template has been borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
