<!-- 
<!DOCTYPE html>
<html> -->

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Coll_Vis_Touch</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf/"/>
    <meta property="og:title" content="mip-NeRF" />
    <meta property="og:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
               Using Collocated Vision and Tactile Sensors <br> for Visual Servoing and Localization
                </br> 
                <small>
                    <!-- IRCA-RAL 22 -->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://jonbarron.info/">
                          Arkadeep Narayan Chaudhury
                        </a>
                        </br>CMU RI
                    </li>
                    <li>
                        <a href="http://bmild.github.io/">
                            Tim Man
                        </a>
                        </br>CMU RI
                    </li>
                    <li>
                        <a href="http://matthewtancik.com/">
                          Wenzhen Yuan
                        </a>
                        </br>CMU RI
                    </li><br>
                    <li>
                        <a href="https://phogzone.com/">
                          Chris Atkeson
                        </a>
                        </br>CMU RI
                    </li>
                   </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/rays.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    A key step in manipulation is estimating the points of contact and  pose of an object with  respect to the robot. In this work we make the point that coordinating tactile sensing with  vision by collocated cameras (instead of only considering the tactile  sensor inputs alone) can 1) provide useful information before contact and 2) simplify the contact point and pose estimation problem. We divide the problem of contact pose estimation into two parts -- the initial phase where the end effector is not in contact with the object and the final phase when the end effector is in contact. We leverage the natural scopes of the cameras and the tactile sensor to visually servo towards the object and obtain a coarse pose estimate of the object with respect to the end effector and refine that estimate to localize the contact using the data obtained from the tactile sensor. Using our ensemble of collocated tactile and visual sensors, we localize contact for objects of a variety of sizes, disambiguate between confusing tactile signals when used for localization, use the optic flow observed by the end effector mounted cameras to generate statistically significant estimates of the direction of heading of the robot and use it to correct robot trajectory errors.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Slides
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTF8WNl8sdtoORXmqUvMbZ3P7SdG8ZqLD58cc56kIGrCCDPW2r79ZOBfRFFjSklwYoljHUeBczknIJD/embed?start=false&loop=false&delayms=15000" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%; mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visual Servoing with Collocated Cameras
                </h3>
                <p class="text-justify">
                    Here, we demonstrate how a pair of collocated cameras can be used to correct trajectory errors on the fly using the optic flow observed by the camera. As a use case, we start with a initial erroneous estimate of the workspace goal (in the world coordinates) and we use the optical flow observed by the cameras en-route to correct against refined estimates of the goal (specified in pixel space by an external agent like a separate algorithm or a human annotator). The key idea behind this is that if the current heading direction is correct, the point of expansion corresponding to the current movement of the cameras should correspond to the workspace goal (in 3d coordinates) imaged by the cameras. We first calculate the point of expansion and then the error (in pixel space) between the current heading direction and the imaged true workspace goal. This pixel space error is then upgraded to the trajectory error using our knowledge of the camera intrinsic and extrinsic parameters (with respect to the robot) and the trajectory error is corrected. 
                </p>
                <p class = "text-justify">
                    We provide the algorithm we used to identify the point in image space corresponding to the momentary heading direction of the robot below. We call this point as the Point of Expansion (POE).
                </p>
               <p style="text-align:center;">
                <image src="./img/POE_algo.png" height="40px" class="img-responsive">
                </p>
                <h3>
                    Correcting trajectory errors with POE estimates
                </h3>
                <p class="text-justify">
                    Let us denote the true workspace goal as \(\mathbf{X}_G^W \) and it's estimate \(\widehat{\mathbf{X}_G^W} \) in homogeneous 3 space. The robot is initially planned to move to \(\widehat{\mathbf{X}_G^W}\) and let the POEs obtained for the subsequent frames be \(\mathbf{p}\), in the pixel space. As the camera is registered to the robot, a given point in time, we can calculate the world to camera projection matrix as \(\mathbf{P} = \mathbf{K}[\mathbf{R}_{world}^{cam}|\mathbf{t}_{world}^{cam}]\)
                    and can project \(\mathbf{X}_G^W\) and it's estimate \(\widehat{\mathbf{X}_G^W}\) to \(\mathbf{x}_G\) and \(\widehat{\mathbf{x}_G}\) in homogeneous 2 space. If the motion between two frames captured by the camera-in-hand is mostly perpendicular to the imaging plane, we can approximate \(\widehat{\mathbf{x}_G}\) with the point of expansion \(\mathbf{p}\) for each subsequent frames. We also note that, as \(\mathbf{p}\) is in the pixel space, with this approximation, we discard the "z-buffer" of the projective transformations associated with \(\mathbf{x}_G\) and as a consequence, cannot correct for a error in the camera's projective axis (camera's Z axis). We  use the camera projection Jacobian and calculate the task-space error as below: 
                    $$
                    \Delta\mathbf{X} = \left[\begin{array}{ccc}
                        \frac{f_x}{Z_W} & 0 & -\frac{X^W_G f_x+Z^W_G c_x}{{Z^W_G}^2}+\frac{c_x}{Z^W_G} \\
                        0 & \frac{f_y}{Z^W_G} & -\frac{Y^W_G f_y+Z^W_G c_y}{{Z^W_G}^2}+\frac{y}{Z^W_G}
                    \end{array} \right]^+ \left[\begin{array}{c}
                    p^u -   \mathbf{x}_G^u   \\
                    p^v - \mathbf{x}_G^v
                    \end{array} \right]
                    $$
                    In the equation above, \([f_x, f_y, c_x, c_y]\) are the camera intrinsics, and we denote \(\mathbf{X}_G^W = [X^W_G, Y^W_G, Z^W_G]\) and the \([\cdot]^+\) denotes the Moore-Penrose pseudo-inverse. We use the following procedure to correct trajectory goals as the robot moves towards the true workspace goal \(\mathbf{X}_G^W\) with a erroneous initial estimate.  
                </p>              


            </div>
        </div>
        
        



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Mip-NeRF
                </h3>
                <p class="text-justify">
                    We use integrated positional encoding to train NeRF to generate anti-aliased renderings. Rather than casting an infinitesimal ray through each pixel, we instead cast a full 3D <em>cone</em>. For each queried point along a ray, we consider its associated 3D conical frustum. Two different cameras viewing the same point in space may result in vastly different conical frustums, as illustrated here in 2D:
                </p>
                <p style="text-align:center;">
                    <image src="img/scales_toy.png" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    In order to pass this information through the NeRF network, we fit a multivariate Gaussian to the conical frustum and use the integrated positional encoding described above to create the input feature vector to the network. 
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p>                
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/ship_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/chair_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/mic_sbs_path1.mp4" type="video/mp4" />
                </video>
                <br><br>
                <p class="text-justify">
                    We can also manipulate the integrated positional encoding by using a larger or smaller radius than the true pixel footprint, exposing the continuous level of detail learned within a single network:
                </p>     
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_radii_manip_slider_200p.mp4" type="video/mp4" />
                </video>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Wikipedia</a> provides an excellent introduction to spatial anti-aliasing techniques.
                </p>
                <p class="text-justify">
                    Mipmaps were introduced by Lance Williams in his paper "Pyramidal Parametrics" (<a href="https://software.intel.com/sites/default/files/m/7/2/c/p1-williams.pdf">Williams (1983)</a>).
                </p>
                <p class="text-justify">
                    <a href="https://dl.acm.org/doi/abs/10.1145/964965.808589">Amanatides (1984)</a> first proposed the idea of replacing rays with cones in computer graphics rendering. 
                </p>
                <p class="text-justify">
                    The closely related concept of <em>ray differentials</em> (<a href="https://graphics.stanford.edu/papers/trd/">Igehy (1999)</a>) is used in most modern renderers to antialias textures and other material buffers during ray tracing.
                </p>
                <p class="text-justify">
                    Cone tracing has been used along with prefiltered voxel-based representations of scene geometry for speeding up indirect illumination calculations in <a href="https://research.nvidia.com/sites/default/files/publications/GIVoxels-pg2011-authors.pdf">Crassin et al. (2011)</a>.
                </p>
                <p class="text-justify">
                    Mip-NeRF was implemented on top of the <a href="https://github.com/google-research/google-research/tree/master/jaxnerf">JAXNeRF</a> codebase.
                </p>
            </div>
        </div>
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
TBD</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank <a href="https://leonidk.com/"> Leonid Keselman </a>,  <a href="https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/">Oliver Kroemer</a>, <a href="https://beisner.me/">Ben Eisner</a>, Arpit Agarwal and <a href="https://sites.google.com/view/rishiv/">Rishi Veerapaneni</a> for several constructive discussions and feedback on the manuscript. This research was partially funded by the NSF.
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
